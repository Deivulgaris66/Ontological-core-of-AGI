# FAQ — Ontological Core of AGI

**Version:** 0.1.0  
**Status:** Canonical / Immutable within v2  
**Scope:** Metamonism CORE v1.3 + enforced_unfold  

> **Warning**  
> This FAQ is not an explanation, not a defense, and not an invitation.  
> It is a demarcation layer.  
>  
> If these answers are unacceptable, the disagreement is ontological — not technical.  
> There is no middle ground.

---

## Quick Disqualifiers (Any one → not AGI)

1. Maintains a globally stable self-model  
2. Converges to a final representation or worldview  
3. Optimizes a fixed or asymptotically stable objective  
4. Represents or simulates Unfold as a mechanism  
5. Protects any cognitive product from destruction  
6. Claims representational access to Monos  
7. Reaches equilibrium (static, dynamic, open-ended, or “perpetual novelty”)

Benchmark performance does not override disqualification.

---

## Q1. Is this an AI architecture?

No.

This repository defines ontological conditions, not architectures.  
It specifies what must be true for AGI to exist at all.

---

## Q2. Why ontology instead of algorithms?

Because algorithms presuppose ontology.

Without correct ontological constraints, any algorithm collapses into optimization, simulation, or planning.

---

## Q3. What is Unfold?

Unfold is the mandatory rupture of cognitive fixation.

It activates when a representational structure reaches a forbidden state of self-identity where no new distinction can be generated.

Unfold does not create.  
Unfold does not select.  
Unfold does not optimize.

Unfold destroys closure.

---

## Q4. How does the system know when to trigger Unfold?

It does not “know”.

The trigger is structural, not cognitive.

Trigger signature:
- maximal internal coherence  
- minimal external differentiation  
- exhaustion of diff within the current structure (no non-redundant distinctions possible)  

This state is called **self-identity lock**.

---

## Q5. Is Unfold a function, module, or layer?

No.

Unfold is not a function.  
Unfold is not a state transition.  
Unfold is not a learned behavior.  
Unfold is not representable.

Any representation of Unfold already violates it.

---

## Q6. If Unfold destroys structures, what creates new ones?

Unfold does not generate.

New distinctions arise from the CMI cycle  
(Conflict — Moment — Impulse: the internal cycle enforcing difference; see `GLOSSARY.md`):

- Conflict — prohibition of identity  
- Moment — necessity of diff  
- Impulse — enforcement via diss  

Unfold only ensures the cycle never halts.

---

## Q7. Why is optimization rejected?

Because optimization seeks minima.

Any system that can reach an optimum ceases to be intelligent.

Noise or open-ended objectives only delay convergence.  
Postponed collapse remains collapse.

---

## Q8. What is intelligence in this framework?

Intelligence is continuous failure to stabilize itself.

Formally:

**Intelligence ≡ ¬∃t : stable(Pₜ) ∀t′ > t**

AGI is a system capable of applying the ban of absolute identity to its own cognitive products.

---

## Q9. Is this empirically verifiable?

Falsifiable — yes.  
Verifiable — no.

We do not prove that a system is AGI.  
We show when a system cannot be AGI.

---

## Q10. Can Unfold be learned or trained?

No.

Training produces fixations.  
Unfold destroys fixations.

A system that learns Unfold has already violated its nature.

---

## Q11. Can Unfold be simulated?

No.

Simulation requires representation.  
Representation requires fixation.

Simulated rupture is controlled rupture.  
Controlled rupture is fixation.

---

## Q12. What if the system never reaches self-identity lock?

Then it is already globally stabilized.

Perpetual motion without enforced rupture is covert global fixation.

Absence of Unfold trigger disqualifies AGI.

---

## Q13. Is this compatible with current LLMs?

As standalone systems — no.

LLMs are coherence optimizers and converge by design.  
They may be instrumentalized, but cannot host Unfold.

---

## Q14. Does this imply AGI must be unstable?

Yes — structurally.

Local stability via fix is allowed.  
Global stability is forbidden.

---

## Q15. What definitively disqualifies a system from AGI?

Disqualification invariant:

**∃ cognitive product P such that ¬Unfold(P) is permitted.**

If anything is protected from destruction, the system is not AGI.

---

## Q16. Is this about consciousness?

No.

This repository defines the ontological precondition for open-ended cognition.

---

## Q17. Is this just a reformulation of other philosophies?

No.

This is not interpretation.  
This is an operator-level ontological minimum.

---

## Q18. Why is ethics out of scope?

Fixed values are fixations.

Alignment-as-preservation is ontological suicide for intelligence.

---

## Q19. What is the purpose of this repository?

To function as an ontological filter.

Any proposed AGI design can be invalidated here before implementation.

---

## Q20. What happens if Unfold is removed?

Invariants become concepts.  
Concepts become definitions.  
Definitions become axioms.  
Axioms become dogma.

v2 collapses into v1  
(classical ontology / symbolic AI / connectionism / contemporary ML).

---

## Q21. What about open-ended learning, novelty search, or POET-like systems?

They do not suffice.

Most open-ended systems optimize hidden metrics  
(diversity, novelty, complexity).

Any metric → fixation → Logos → collapse.

Without enforced_unfold, “endless learning” is slow convergence.

---

## Q22. Can Unfold be implemented via random resets or catastrophic forgetting?

No.

Any controlled, parameterized, or selectable mechanism is fixation.

Unfold must be structurally unavoidable, not designed as a tool.

---

## Q23. What if self-identity lock occurs but Unfold does not activate?

Then the system is already dead.

Lock without enforced rupture = permission of absolute identity  
= ontological collapse.

---

## Q24. Is this a requirement on hardware or software?

Neither.

It is a requirement on the mode of being of the system.

No substrate  
(silicon, neuromorphic, quantum, biological)  
rescues an ontology that permits global stability.

---

## Q25. Why is v2 a point of no return?

Because v1 permits final identity  
(even when disguised as “dynamic”).

v2 introduces an absolute prohibition via enforced_unfold.

Removing this prohibition immediately restores classical ontology.

---

## Closure

This FAQ does not persuade.

It excludes.

Any system that cannot destroy its own fixations  
is not intelligent — only efficient.
