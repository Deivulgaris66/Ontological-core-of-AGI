# FAQ — Ontological Core of AGI

**Version:** 0.1.0  
**Status:** Canonical / Immutable within v2  
**Scope:** Metamonism CORE v1.3 + enforced_unfold  

> **Warning**  
> This FAQ is not an explanation, not a defense, and not an invitation.  
> It is a demarcation layer.  
>  
> If these answers are unacceptable, the disagreement is ontological — not technical.  
> There is no middle ground.

---

## Quick Disqualifiers

A system is **not AGI** if **any** of the following holds:

1. It maintains a globally stable self-model.
2. It converges toward any final representation or worldview.
3. It optimizes a fixed or asymptotically stable objective.
4. It represents or simulates Unfold as an internal mechanism.
5. It protects any cognitive product from destruction.
6. It claims access to Monos through representation.
7. It reaches equilibrium (even dynamic equilibrium).

Benchmark performance does not override disqualification.

---

## Q1. Is this an AI architecture?

No.

This repository defines ontological conditions, not architectures.  
It specifies what must be true for AGI to exist at all.

---

## Q2. Why ontology instead of algorithms?

Because algorithms presuppose ontology.

Without correct ontological constraints, any algorithm collapses into optimization, simulation, or planning.

---

## Q3. What is Unfold?

Unfold is the mandatory rupture of cognitive fixation.

It activates when a representational structure reaches a forbidden state of self-identity where no new distinction can be generated.

Unfold does not create.  
Unfold does not select.  
Unfold does not optimize.  

Unfold destroys closure.

---

## Q4. How does the system know when to trigger Unfold?

It does not “know”.

The trigger is structural, not cognitive.

Trigger signature:
- maximal internal coherence
- minimal external differentiation
- impossibility of further diff without redundancy or contradiction

This state is called **self-identity lock**.

---

## Q5. Is Unfold a function, module, or layer?

No.

Unfold is not a function.  
Unfold is not a state transition.  
Unfold is not a learned behavior.  
Unfold is not representable.

Any representation of Unfold already violates it.

---

## Q6. If Unfold destroys structures, what creates new ones?

Unfold does not generate.

New distinctions arise from the CMI cycle:

- Conflict — prohibition of identity  
- Moment — necessity of diff  
- Impulse — enforcement via diss  

Unfold only ensures the cycle never halts.

---

## Q7. Why is optimization rejected?

Because optimization seeks minima.

Any system that can reach an optimum ceases to be intelligent.

Noise or open-ended objectives only delay convergence.  
Delayed collapse is still collapse.

---

## Q8. What is intelligence in this framework?

Intelligence is continuous failure to stabilize itself.

Formally:  
AGI is a system capable of applying the ban of absolute identity to its own cognitive products.

---

## Q9. Is this empirically verifiable?

Falsifiable — yes.  
Verifiable — no.

We do not prove that a system is AGI.  
We show when a system cannot be AGI.

---

## Q10. Can Unfold be learned or trained?

No.

Training produces fixations.  
Unfold destroys fixations.

A system that learns Unfold has already violated its nature.

---

## Q11. Can Unfold be simulated?

No.

Simulation requires representation.  
Representation requires fixation.  

Simulated rupture is controlled rupture.  
Controlled rupture is fixation.

---

## Q12. What if the system never reaches self-identity lock?

Then it is already globally stabilized.

Perpetual change without rupture is still fixation.

Absence of Unfold trigger disqualifies AGI.

---

## Q13. Is this compatible with current LLMs?

As standalone systems — no.

LLMs are coherence optimizers and converge by design.  
They may be instrumentalized, but cannot host Unfold.

---

## Q14. Does this imply AGI must be unstable?

Yes — structurally.

Local stability via fix is allowed.  
Global stability is forbidden.

---

## Q15. What definitively disqualifies a system from AGI?

Disqualification invariant:

∃ cognitive product P such that ¬Unfold(P) is permitted.

If anything is protected from destruction, the system is not AGI.

---

## Q16. Is this about consciousness?

No.

This repository defines the ontological precondition for open-ended cognition.

---

## Q17. Is this just a reformulation of other philosophies?

No.

This is not interpretation.  
This is an operator-level ontological minimum.

---

## Q18. Why is ethics out of scope?

Fixed values are fixations.

Alignment through preservation contradicts intelligence.

---

## Q19. What is the purpose of this repository?

To function as an ontological filter.

Any proposed AGI design can be invalidated here before implementation.

---

## Q20. What happens if Unfold is removed?

Invariants become concepts.  
Concepts become definitions.  
Definitions become axioms.  
Axioms become dogma.

v2 collapses into v1.

---

## Closure

This FAQ does not persuade.

It excludes.

Any system that cannot destroy its own fixations  
is not intelligent — only efficient.
