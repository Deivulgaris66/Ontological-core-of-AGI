FAQ — Ontological Core of AGI

Version: 0.1.0
Status: Canonical / Immutable within v2
Scope: Metamonism CORE v1.3 + enforced_unfold

Warning
This FAQ is not an explanation, not a defense, and not an invitation.
It is a demarcation layer.

If these answers are unacceptable, the disagreement is ontological — not technical.
There is no middle ground.

Quick Disqualifiers (Read First)

A system is not AGI if any of the following holds:

It maintains a globally stable self-model.

It converges toward any final representation or worldview.

It optimizes a fixed or asymptotically stable objective.

It represents or simulates Unfold as an internal mechanism.

It protects any cognitive product from destruction.

It claims access to Monos through representation.

It reaches equilibrium (even dynamic equilibrium).

Passing benchmarks does not override disqualification.

Q1. Is this an AI architecture?

No.

This repository defines ontological conditions, not architectures.

It specifies what must be true for AGI to exist at all —
not how to build a system.

Q2. Why ontology instead of algorithms?

Because algorithms presuppose ontology.

Without correct ontological constraints, any algorithm — regardless of scale — collapses into optimization, simulation, or planning.

AGI fails before implementation if ontology is wrong.

Q3. What is Unfold?

Unfold is the mandatory rupture of cognitive fixation.

It is activated when a representational structure reaches a forbidden state of self-identity, where no new distinction can be generated within the current paradigm.

Unfold does not create.
Unfold does not select.
Unfold does not optimize.

Unfold destroys closure.

Q4. How does the system know when to trigger Unfold?

It does not “know”.

The trigger is structural, not cognitive.

Trigger condition (signature):

maximal internal coherence

minimal external differentiation

inability to produce new distinctions (diff) without redundancy or contradiction

This configuration is called self-identity lock.

Triggering is not a decision.
It is a forced transition — like a phase change.

Q5. Is Unfold a function, module, or layer?

No.

Unfold is:

not a function

not a state transition

not a learned behavior

not a controllable mechanism

Any representation of Unfold already violates it.

Q6. If Unfold destroys structures, what generates new ones?

Unfold does not generate.

New distinctions arise from the CMI cycle:

Conflict — prohibition of absolute identity

Moment — necessity of differentiation (diff)

Impulse — enforcement via dissipation (diss)

Unfold only ensures that this cycle cannot halt.

Q7. Why is optimization explicitly rejected?

Because optimization seeks minima.

Any system that can reach an optimum — global or local — ceases to be intelligent.

Intelligence exists only in non-equilibrium states.

Noise, stochasticity, or open-ended objectives do not solve this:
they merely delay convergence.

Delayed collapse is still collapse.

Q8. What is intelligence in this framework?

Intelligence is continuous failure to stabilize itself.

Formally:

AGI is a system capable of applying the ban of absolute identity
to its own cognitive products.

If a system can preserve any invariant indefinitely, it is not intelligent.

Q9. Is this empirically verifiable?

Falsifiable — yes.
Verifiable — no.

We do not prove that a system is AGI.

We show when a system cannot be AGI, regardless of performance.

This is an ontological filter, not a benchmark.

Q10. Can Unfold be learned or trained?

No.

Training produces fixations.

Unfold destroys fixations.

A system that “learns” Unfold has already violated its nature.

Q11. Can Unfold be simulated (e.g. random resets, forgetting)?

No.

Simulation requires representation.
Representation requires fixation.

Simulated rupture is controlled rupture.
Controlled rupture is fixation.

This is an ontological contradiction.

Q12. What if the system never reaches self-identity lock?

Then it is already in a forbidden state of global fixation.

“Perpetual change” without rupture is still stabilization.

Absence of Unfold trigger disqualifies AGI.

Q13. Is this compatible with current LLMs?

As standalone systems — no.

LLMs are ontologically v1:

coherence optimizers

fixation-preserving

convergence-driven

They may be instrumentalized within a higher processual system,
but they cannot host Unfold themselves.

Q14. Does this imply AGI must be unstable?

Yes — structurally.

Local stability (via fix) is allowed.
Global stability is forbidden.

This is not randomness.
This is enforced non-equilibrium.

Q15. What definitively disqualifies a system from AGI?

Formal disqualification invariant:

∃ cognitive product P such that ¬Unfold(P) is permitted.

If anything is protected from destruction, the system is not AGI.

Q16. Is this about consciousness?

No.

This repository defines the ontological precondition for open-ended cognition.

Whether consciousness arises is a separate question.

Q17. Is this just a reformulation of Deleuze / Bergson / Gödel?

Partially overlapping, but categorically different.

This is not interpretation.

This is an operator-level minimum:
a set of prohibitions without which AGI is impossible.

Q18. Why is ethics out of scope?

Because fixed values are fixations.

Alignment through preservation is incompatible with intelligence.

Ethics, if possible, must be processual — not axiomatic.

This lies above the current layer.

Q19. What is the purpose of this repository?

To function as an ontological filter.

Any proposed AGI design can be tested here before implementation.

If it violates these constraints, it is guaranteed to collapse.

Q20. What happens if Unfold is removed?

Then:

invariants become concepts

concepts become definitions

definitions become axioms

axioms become dogma

v2 collapses into v1.

Classical AI reappears.

Closure

This FAQ does not persuade.

It excludes.

Any system that cannot destroy its own fixations
is not intelligent — only efficient
